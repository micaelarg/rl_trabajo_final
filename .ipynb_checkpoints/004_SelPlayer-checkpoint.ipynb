{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027229e4-9c75-4418-9111-3d1afc4c5526",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1674766275365,
     "user": {
      "displayName": "Micaela Kulesz",
      "userId": "06569199278012136105"
     },
     "user_tz": 180
    },
    "id": "027229e4-9c75-4418-9111-3d1afc4c5526",
    "outputId": "93a1530e-b72b-4d80-be0c-6a3c6489dcc9"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d47e401-bb8b-4200-8aa9-afaa856a924b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "error",
     "timestamp": 1674766064531,
     "user": {
      "displayName": "Micaela Kulesz",
      "userId": "06569199278012136105"
     },
     "user_tz": 180
    },
    "id": "0d47e401-bb8b-4200-8aa9-afaa856a924b",
    "outputId": "6b653acb-8722-43f9-e99f-8f66a2c99e2e"
   },
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv\n",
    "import numpy as np\n",
    "from players import RandomPlayer\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74717cb-c130-4894-99ec-548d0f39acf3",
   "metadata": {
    "id": "b74717cb-c130-4894-99ec-548d0f39acf3"
   },
   "source": [
    "# SelfPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f09c2-5a93-4a5e-9e53-a11391af1cea",
   "metadata": {
    "id": "cd3f09c2-5a93-4a5e-9e53-a11391af1cea"
   },
   "source": [
    "En esta notebook se pide armar un entorno al cual se le pase como parámetro la clase de jugador local (DictPlayer, RandomPlayer, GreedyPlayer), y que el entorno devuelva el siguiente paso luego de jugar con el jugador local. Algunas condiciones:\n",
    "- En la función de reset(), se sortearea si el jugador local juega primero o segundo. \n",
    "- El entorno siempre devolverá el tablero como si le tocará jugar al jugador 1. Sea primero o segundo\n",
    "- La clase se instancia con los siguientes parámetros:\n",
    "    - LocalPlayer\n",
    "    - board_shape\n",
    "    \n",
    "- El método step recibirá como parámtro la acción pero codificada no como action = [columna, fila], si no como: action = action[0] * board_shape + action[1]\n",
    "- self.action_space tiene que estar definido acorde al espacio de acción. Por ejemplo: self.action_space = gym.spaces.Discrete(board_shape**2)\n",
    "- self.observation_space también: self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c38839c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([[[-1. -1. -1. -1. -1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
       "  [-1. -1. -1. -1. -1. -1. -1. -1.]]], [[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1. 1. 1. 1.]]], (1, 8, 8), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.spaces.Box(-1, 1, (1, 8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d97d4-db5e-46b7-a273-4fad6bc07373",
   "metadata": {
    "id": "2d1d97d4-db5e-46b7-a273-4fad6bc07373"
   },
   "source": [
    "# Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03553fc6-987c-4e88-9cf3-777cd4b87843",
   "metadata": {
    "id": "03553fc6-987c-4e88-9cf3-777cd4b87843"
   },
   "source": [
    "El jugador local juega segundo entonces el reset() devuelve (Notar que no se devuelve el player por que siempre juega el 1):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a50ac17-cfa0-452b-9fa2-63a495ffbe6f",
   "metadata": {
    "id": "5a50ac17-cfa0-452b-9fa2-63a495ffbe6f"
   },
   "source": [
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090f893-6ede-4b8d-9559-b01341181dee",
   "metadata": {
    "id": "3090f893-6ede-4b8d-9559-b01341181dee"
   },
   "source": [
    "El jugador local juega primero entonces el reset() devuelve:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cfa1d6c-c465-46a5-b539-56edd8b6513c",
   "metadata": {
    "id": "7cfa1d6c-c465-46a5-b539-56edd8b6513c"
   },
   "source": [
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac37d6d6-e177-4cc7-b1f3-f136e6bb40fd",
   "metadata": {
    "id": "ac37d6d6-e177-4cc7-b1f3-f136e6bb40fd"
   },
   "source": [
    "Que ocurrio aca?\n",
    "\n",
    "El tablero se resetea y queda:\n",
    "\n",
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Luego el jugador local muestrea una de las cuatros opciones válidas y juega (0, 2) comiendo la pieza (1, 2) y tranformandola en 1\n",
    "\n",
    "[[ 0,  0,  1,  0],\n",
    " [ 0,  1,  1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora le toca el turno al jugador -1 pero el jugador externo tiene que ver el tablero como si fuera el 1, entonces se multiplica el tablero por -1\n",
    "\n",
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora el jugador externo seleccionará una acción observando el tablero como si fuera 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f918a12-4d72-48cb-84b0-617054ec02a3",
   "metadata": {
    "id": "5f918a12-4d72-48cb-84b0-617054ec02a3"
   },
   "source": [
    "En cuanto a la recompenza tener en cuenta que deberá devolver:\n",
    "- 1 si gana el jugador externo\n",
    "- -1 si gana el LocalPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef1f596-1a0e-449f-86f1-5c5115a00d17",
   "metadata": {
    "id": "bef1f596-1a0e-449f-86f1-5c5115a00d17"
   },
   "outputs": [],
   "source": [
    "class SelfPlayEnv(ReversiEnv):\n",
    "    def __init__(self, board_shape=8, LocalPlayer=None):\n",
    "        self.players = [-1, 1]\n",
    "\n",
    "        self.local_player = LocalPlayer(board_shape=board_shape, flatten_action=False)\n",
    "        self.board_shape = board_shape\n",
    "        super(SelfPlayEnv, self).__init__(board_shape=board_shape)\n",
    "        \n",
    "        self.action_space =  gym.spaces.Discrete(board_shape**2)\n",
    "        self.observation_space =  gym.spaces.Box(-1, 1, (1, board_shape,board_shape))\n",
    "         \n",
    "        \n",
    "    def play(self, observation):\n",
    "        action = self.local_player.predict(observation)\n",
    "        (observation, self.current_player_num), reward, done, info = super(SelfPlayEnv, self).step(action)\n",
    "\n",
    "        return (observation, self.current_player_num), reward, done, info\n",
    "    \n",
    "    def encode_observation(self, observation, valid_actions=False):\n",
    "        # Implementar\n",
    "        # Simpre devuelve desde el punto de vista del jugador 1\n",
    "        # No devueleve el jugador sino solo el tablero\n",
    "        # Tener en cuenta que esto será la entrada a la red neuronal\n",
    "        board = observation * self.current_player_num\n",
    "        if valid_actions:\n",
    "            return np.array([board, self.get_valid((board, 1))])\n",
    "        else:\n",
    "            return observation * self.current_player_num\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_step = 0\n",
    "        self.local_player_num = np.random.choice(self.players)\n",
    "        self.local_player.player = self.local_player_num\n",
    "        self.observation, self.current_player_num = super(SelfPlayEnv, self).reset()\n",
    "        self.allow_pass = True\n",
    "\n",
    "            \n",
    "        if self.current_player_num == self.local_player_num:   \n",
    "            (self.observation, self.current_player_num), _, done, info = self.play(self.observation)\n",
    "            assert done == False\n",
    "\n",
    "        \n",
    "        return self.encode_observation(self.observation)\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        # Esta es necesario ya que la salida de la red neuronal será un valor entre 0 y board_shape**2 - 1\n",
    "        return [action // self.board_shape, action % self.board_shape]\n",
    "    \n",
    "    def decode_action(self, action):\n",
    "        return action[0] * self.board_shape + action[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.n_step += 1\n",
    "        action = self.encode_action(action)\n",
    "        \n",
    "        (self.observation, self.current_player_num), reward, done, _ = super(SelfPlayEnv, self).step(action)   \n",
    "        \n",
    "            \n",
    "        while not done and (self.current_player_num == self.local_player_num):            \n",
    "            (self.observation, self.current_player_num), reward, done, info = self.play(self.observation)\n",
    "\n",
    "        \n",
    "        encoded_observation = self.encode_observation(self.observation)\n",
    "        reward =  -float(self.local_player_num*reward)\n",
    "\n",
    "        return encoded_observation, reward, done, {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce3799d-d2bd-4590-b076-25e742c235f3",
   "metadata": {
    "id": "1ce3799d-d2bd-4590-b076-25e742c235f3"
   },
   "outputs": [],
   "source": [
    "env = SelfPlayEnv(board_shape=4, LocalPlayer=RandomPlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab4a7bd-0b0c-47e4-a63f-465fa83226de",
   "metadata": {
    "id": "bab4a7bd-0b0c-47e4-a63f-465fa83226de",
    "outputId": "d5acaaac-7329-470a-d6a7-5d46f2bff3f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0, -1,  1,  0],\n",
       "       [ 0, -1, -1,  0],\n",
       "       [ 0, -1,  0,  0]], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c2352-7ff1-455c-b2e8-b0a9844dd1f7",
   "metadata": {
    "id": "da0c2352-7ff1-455c-b2e8-b0a9844dd1f7"
   },
   "source": [
    "# Probar entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d80054a-c061-4973-9a95-ab76fa91a693",
   "metadata": {
    "id": "5d80054a-c061-4973-9a95-ab76fa91a693"
   },
   "outputs": [],
   "source": [
    "def sample_valid_actions(state):\n",
    "    # np.argwhere junto con env.get_valid y randint solucionan el problema en pocas lineas pero puede usar otra estrategia\n",
    "    board_shape = state.shape[0]\n",
    "    # El player es siempre 1\n",
    "    player = 1\n",
    "    valid_actions = np.argwhere(env.get_valid((state, player)) == 1)\n",
    "    action = valid_actions[np.random.randint(len(valid_actions))]\n",
    "    return action[0] * board_shape + action[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f074437d-a34d-4bdf-a122-164c1ee63123",
   "metadata": {
    "id": "f074437d-a34d-4bdf-a122-164c1ee63123",
    "outputId": "a581816b-64ff-4fd6-af5d-3dc49a9515d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "acion: 7\n",
      "[[ 0  0  0 -1]\n",
      " [ 0  1 -1  1]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "acion: 2\n",
      "[[ 0  0  1 -1]\n",
      " [ 0  1  1 -1]\n",
      " [ 0 -1 -1 -1]\n",
      " [ 0  0  0  0]]\n",
      "acion: 15\n",
      "[[ 0  0  1 -1]\n",
      " [-1 -1 -1 -1]\n",
      " [ 0 -1  1 -1]\n",
      " [ 0  0  0  1]]\n",
      "acion: 0\n",
      "[[ 1  0  1 -1]\n",
      " [-1  1 -1 -1]\n",
      " [ 0 -1 -1 -1]\n",
      " [ 0 -1  0  1]]\n",
      "acion: 8\n",
      "[[ 1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1]\n",
      " [ 0 -1  0  1]]\n",
      "acion: 14\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "board = env.reset()\n",
    "while not done:\n",
    "    action = sample_valid_actions(board)\n",
    "    print(board)\n",
    "    print(f'acion: {action}')\n",
    "    \n",
    "    board, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc2a60-1484-43cf-9cfe-3bc435aaa548",
   "metadata": {
    "id": "63cc2a60-1484-43cf-9cfe-3bc435aaa548"
   },
   "source": [
    "# Entornos vectoriales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce9342c-4c16-4fa2-aed7-a591e4fad79c",
   "metadata": {
    "id": "0ce9342c-4c16-4fa2-aed7-a591e4fad79c"
   },
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dfb931b-9602-489f-b52c-4266b054b78c",
   "metadata": {
    "id": "7dfb931b-9602-489f-b52c-4266b054b78c"
   },
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb45e843-724f-4318-942e-7b581909d973",
   "metadata": {
    "id": "fb45e843-724f-4318-942e-7b581909d973",
    "outputId": "b97c2245-f7e6-49f6-ffba-ab93aa13da72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172549d-264f-4e31-bba9-939f5a1cd61d",
   "metadata": {
    "id": "6172549d-264f-4e31-bba9-939f5a1cd61d"
   },
   "source": [
    "- Notar que la entrada tiene como primer componente la cantidad de entornos en paralelo (10), luego la cantidad de canales (1), y finalmente las dimensiones del tablero \n",
    "\n",
    "- Imprimir obs y ver que hay distintas posibles entradas dependiendo de quien juega primero y que jugó el LocalPlayer si le toco primero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c95848-d644-435e-af95-cf754690dfa1",
   "metadata": {
    "id": "36c95848-d644-435e-af95-cf754690dfa1"
   },
   "source": [
    "### Guardar el SelfPlayEnv en el módulo multi_env para poder despues importarla desde otra notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c447b-def1-46e6-8bf3-299e7e596d2c",
   "metadata": {
    "id": "685c447b-def1-46e6-8bf3-299e7e596d2c"
   },
   "source": [
    "# Instanciamos el modelo con MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16ebead4-efc5-498e-a411-e8f1ef955ec8",
   "metadata": {
    "id": "16ebead4-efc5-498e-a411-e8f1ef955ec8"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ecdb1b-7588-488b-b4ed-5722f2fb2555",
   "metadata": {
    "id": "04ecdb1b-7588-488b-b4ed-5722f2fb2555",
    "outputId": "e7fef4e2-b758-477f-e01c-3bfa2a7c120e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ac1fd0-947c-4e8d-b42a-5b18ffc3f687",
   "metadata": {
    "id": "38ac1fd0-947c-4e8d-b42a-5b18ffc3f687"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd324ce9-6a80-437e-aedd-b44585c99afa",
   "metadata": {
    "id": "dd324ce9-6a80-437e-aedd-b44585c99afa"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c274276c-25eb-4b94-9841-89e00795fe49",
   "metadata": {
    "id": "c274276c-25eb-4b94-9841-89e00795fe49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6736603d-2624-4e97-8d32-dcb251ff1586",
   "metadata": {
    "id": "6736603d-2624-4e97-8d32-dcb251ff1586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([62, 51, 44,  7, 31, 31, 27, 14, 34, 35]), None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497081e-e277-4dff-b576-ae58f96ac85b",
   "metadata": {
    "id": "1497081e-e277-4dff-b576-ae58f96ac85b"
   },
   "source": [
    "Observaciones:\n",
    "- Lo primero que hace stablebaselines si ponemos MLP es un flatten\n",
    "- Las acciones predichas por el modelo (sin entrentar) tienen una alta probabildad de ser inválidas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a0c2b16f467ec78db051111ddafeba9e94633fed46fe39706ae157a4f143e59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
